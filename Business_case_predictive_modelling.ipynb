{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f725f0f2",
   "metadata": {},
   "source": [
    "# Business Case Report, Tymoteusz Cie≈õlik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480ce7f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0d47e",
   "metadata": {},
   "source": [
    "Everytime a company enters a new market in a different country there emerges a need to analyse the circumstances, environment and the potential client target. No matter the size or recognition, the business is set to generate profits while distributing different services. In order to provide that for the gym chain, it is essential to reach the people that would be more or less interested with what the particular company can offer. In this case we have to deal with the information about people stored in different files and the indication whether they would be willing to buy a long--term gym subscription. Our goal is to build a predictive model based on the provided training data in order to foresee the potential targets in the test data. At first we should import the necessary libraries and modules that would allow to solve the posed problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b864097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as skm\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import catboost as ct\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac5e5a",
   "metadata": {},
   "source": [
    "## Input and data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac70d15",
   "metadata": {},
   "source": [
    "In the next step we are able to load the training and test data into python instance. For each group there exist two files:\n",
    "- one .csv file where the data is structurally stored in the form of a table and for each person there are variables representing their characteristics and their personal information,\n",
    "- one .json file where the data is unstructured and stored in a form of nested dictionaries.\n",
    "\n",
    "What matters the most is the data cleaning and ordering so as it could be used as an input to a predictive model. In case of the numbers, the training data contains 4000 entries while the test one consists of half of this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf11e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = json.load(open('train.json', encoding='utf-8'))\n",
    "data_json_test = json.load(open('test.json', encoding='utf-8'))\n",
    "\n",
    "df_csv = pd.read_csv(r'train.csv')\n",
    "df_csv_test = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6939289",
   "metadata": {},
   "source": [
    "At first, we can focus on the data in .json file. As we have mentioned it does not have a structure, so it is crucial to retrieve the information for each surveyed person. In general it contains the groups on some social media platform, which the particular people are a part of. By iterating over the file we are able to find the information about each person indicated by its 'id' and create the data frame where each column represents one particular group. During this process we can omit the city name appearing in parentheses and indicating the area where the person operates from, as it does not provide any useful information and could only unnecessarily increase the dimensionality of the data. Then for the purpose of the predictive modelling, we transorm the data frame by imposing the onehot encoding algorithm so that instead of categorical attributes we are able to store the information as the binary indication whether each user belongs to a group or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094b24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_processing(data_json):\n",
    "    \"\"\"\n",
    "    The function is used to process the data stored in.json file which is considered as the argument. \n",
    "    It cleans the information stored inside and returns it as a pandas DataFrame, where there was applied onehot encoding\n",
    "    in order for the categorical variables to be preserved and used for predictive modelling.\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for i in range(len(data_json['data'])):\n",
    "        person_groups=[]\n",
    "        grp = len(data_json['data'][i]['groups']['data'])\n",
    "        if grp == 0:\n",
    "            groups.append([int(data_json['data'][i]['id']),['no group']])\n",
    "        else:\n",
    "            for group in range(grp):\n",
    "                single_g = data_json['data'][i]['groups']['data'][group]['group_name']\n",
    "                single_group = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", single_g)\n",
    "                person_groups.append(single_group)\n",
    "            groups.append([int(data_json['data'][i]['id']),person_groups])\n",
    "    df_groups = pd.DataFrame(groups,columns=['id', 'groups'])\n",
    "    df_groups = df_groups.set_index('id')\n",
    "    df_groups_onehot = pd.get_dummies(df_groups.groups.apply(pd.Series).stack()).sum(level=0)\n",
    "    return df_groups_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b52e8",
   "metadata": {},
   "source": [
    "When it comes to the .csv file, it appears that the data transformation should not be complicated as it is already structured. One thing that catches an eye is the hobbies variable, where instead of a string we have the list of hobbies for each person. Similarly as before, we are able to create a new data frame, where each column represents one particular hobby for each entry. After that once more, we transform it in a way where we use onehot encoding in order for the variables to be binary and the possibility of using them in predictive modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e07b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hobbies_processing(data_csv):\n",
    "    \"\"\"\n",
    "    The function operates on a .csv file and transforms the hobbies column into a separate frame, where each column\n",
    "    represents one hobby. Then the onehot encoding is used for the predictive purposes and the transformed data frame\n",
    "    is the output of the fuction.\n",
    "    \"\"\"\n",
    "    hobbies=[]\n",
    "    for ids, hobby in data_csv[['user_id','hobbies']].itertuples(index=False):\n",
    "        try:\n",
    "            more_hobbies = hobby.split(\",\")\n",
    "            hobby_l = []\n",
    "            for h in more_hobbies:\n",
    "                hobby_l.append(h.lower())\n",
    "            hobbies.append([ids,hobby_l])\n",
    "        except:\n",
    "            hobbies.append([ids,'no hobby'])\n",
    "    df_hobbies = pd.DataFrame(hobbies,columns=['id', 'hobbies'])\n",
    "    df_hobbies = df_hobbies.set_index('id')\n",
    "    df_hobbies_onehot = pd.get_dummies(df_hobbies.hobbies.apply(pd.Series).stack()).sum(level=0)\n",
    "    return df_hobbies_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863d62d",
   "metadata": {},
   "source": [
    "For the remaining information stored in a .csv file we could still perform some tranformation in order to enhance their performation in predictions. For example we discretize the date of birth of each person assinging the decade of their birth instead of the particular day, as it is enough to assess the person's age. Then we divide the whole table into two groups of varibles, the numerical ones and the categorical ones:\n",
    "- for numerical variables we perform their normalization, in order for each variable's values to be in range [0,1], so that each attribute could contribute equally to the model, \n",
    "- for categorical variables we once more apply onehot encoding so that the information is preserved as binary indication.\n",
    "\n",
    "Besides that, during the process we omit the information that should not contribute to the models, such as name and the locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9449a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data):\n",
    "    \"\"\"\n",
    "    The function takes as the input the csv file and processes the data inside in order for it to be used for predictive\n",
    "    modelling. By transforming the date of birth into a decade, dividing the variales into categorical and numerical ones and \n",
    "    normalizing them it prepares two data frames with attributes that then can contribute to the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    decade = []\n",
    "    for date in data['dob']:\n",
    "        try:\n",
    "            if int(date[0:4]) < 1930:\n",
    "                decade.append(\"1920s\")\n",
    "            elif int(date[0:4]) >= 1930 and int(date[0:4]) < 1940:\n",
    "                decade.append(\"1930s\")\n",
    "            elif int(date[0:4]) >= 1940 and int(date[0:4]) < 1950:\n",
    "                decade.append(\"1940s\")\n",
    "            elif int(date[0:4]) >= 1950 and int(date[0:4]) < 1960:\n",
    "                decade.append(\"1950s\")\n",
    "            elif int(date[0:4]) >= 1960 and int(date[0:4]) < 1970:\n",
    "                decade.append(\"1960s\")\n",
    "            elif int(date[0:4]) >= 1970 and int(date[0:4]) < 1980:\n",
    "                decade.append(\"1970s\")\n",
    "            elif int(date[0:4]) >= 1980 and int(date[0:4]) < 1990:\n",
    "                decade.append(\"1980s\")\n",
    "            elif int(date[0:4]) >= 1990 and int(date[0:4]) < 2000:\n",
    "                decade.append(\"1990s\")\n",
    "            elif int(date[0:4]) >= 2000:\n",
    "                decade.append(\"00s\")\n",
    "        except:\n",
    "            decade.append(date)\n",
    "    data['decade_of_birth'] = decade\n",
    "    numerical_csv = data[['location_population', 'location_from_population', 'daily_commute', 'friends_number', 'education']]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    numerical_csv_norm = scaler.fit_transform(numerical_csv)\n",
    "    numerical_csv_norm = pd.DataFrame(numerical_csv_norm, columns = numerical_csv.columns)\n",
    "    onehot_csv = pd.get_dummies(data=data[['sex','decade_of_birth', 'occupation', 'relationship_status','credit_card_type']])\n",
    "    return onehot_csv, numerical_csv_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3af170",
   "metadata": {},
   "source": [
    "After the necessary transformations we can call the functions on the training data in order to obtain four different data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9276527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups_onehot_train = group_processing(data_json)\n",
    "df_hobbies_onehot_train = hobbies_processing(df_csv)\n",
    "onehot_csv_train, numerical_csv_norm_train = data_processing(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ef834",
   "metadata": {},
   "source": [
    "Then, we merge all of them so that we will have one big data frame containing all information that could be used for predictive modelling. It's worth noticing that in the data there are a lot of missing values what can negatively impact the quality of predictions, so that as the potential help we can create a separate frame containing only rows where the value is provided for each column. In addition, we have decided not to impute the missing values, as it would not make sense because each individual is different and has its preferences so that averaging the values is pointless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010ea776",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([numerical_csv_norm_train, df_groups_onehot_train,df_hobbies_onehot_train,onehot_csv_train],axis = 1)\n",
    "merged_no_nan = merged.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac87dcb",
   "metadata": {},
   "source": [
    "As we have prepared all the necessary information we can move to the predictions themselves. Before that it's important to understand with what kind of problem we have to deal with. Basing only on the training data we will have to predict the binary label of the target. That way we can claim that the issue beongs to the group of supervised learning problems, where we are bound to use the classification method due to the fact that the target is a categorical variable. That way we can move on to the presentation of the solution of a posed problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acf262",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d2d34",
   "metadata": {},
   "source": [
    "The first thing to do is the selection of way of dealing with the problem. In order to have a bigger picture of it and the variety of results to be wider we can use different machine learning models imposed on different subsets of features. Then through implementing different metrics of model accuracy evaluation we will be able to choose the optimal combination and use it to predict the labels on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d2568",
   "metadata": {},
   "source": [
    "### Subsets of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f901e",
   "metadata": {},
   "source": [
    "The main goal of the predictive modeling is to achieve as big accuracy of the classification as possible so that we can increase the probability of success by using different subsets of features for this task. We decided that there would be three possibilities to apply:\n",
    "- the first subset will be the whole data frame which was obtained in previous steps, as that way it's possible to assess the accuracy when all variables are concerned,\n",
    "- the second subset will contain ten automatically chosen variables by implenenting  SelectKBest function form the sklearn module, which  performs univariate linear regression tests to choose the most influential attributes which then make up for the subset. The drawback of such approach is that we can only use this approach on the table where there are no missing values,\n",
    "- for the last one we can use a dimensionality reduction method in order to reduce the number of columns that contribute to the whole model. Such application can drastically change the values of columns but preserves the whole information so that it is useful to analyse the performance of such subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "459ce493",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df_csv['target'])\n",
    "labels_no_nan = df_csv.iloc[list(merged_no_nan.index)]['target']\n",
    "features = np.array(merged)\n",
    "features_no_nan = np.array(merged_no_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc52482",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsc_10 = SelectKBest(score_func=f_classif, k=10)\n",
    "features_selected_10 = fsc_10.fit_transform(features_no_nan, labels_no_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4aff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA() \n",
    "features_pca = pca.fit_transform(features_no_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b6d22",
   "metadata": {},
   "source": [
    "### Quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b874f",
   "metadata": {},
   "source": [
    "In these part we can present 4 different model evaluation metrics that would help us decide which combination of ML method and subset performs the best. The metrics include:\n",
    "- Accuracy, as the main indicator of the fraction of the labels that were correctly assigned,\n",
    "- Area Under ROC Curve, which allows measuring the ability of a classifier to distinguish between classes,\n",
    "- precision and recall that describe the fraction of relevant instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3c440b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorings_class = ['accuracy', 'roc_auc', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fc95c",
   "metadata": {},
   "source": [
    "### Machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3e1cd",
   "metadata": {},
   "source": [
    "Now, when we know the subsets and the way of evaluating the created models we can move to the presentation of methods that we will use for the problem. As in the data there are a lot of missing values and we are not in favor of dropping any rows containing them, we should use the ones that deal well with such obstacles and provide a good performance regardless of their drawbacks.\n",
    "\n",
    "We've decided to use and implement three different ML algorithms/methods, which are as follows:\n",
    "- XGBoost,\n",
    "- Catboost,\n",
    "- LightGBM.\n",
    "\n",
    "They are the state-of-the art algorithms that can be used as classifiers and do not pose any restrictions that would exclude some of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9349d",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed1004",
   "metadata": {},
   "source": [
    "For all of the created models we will use 5-fold cross validation in order for the training set to be used entirely and consistently. That way we will be able to average the obtained results and the values of quality evaluations, what would make the classifiers more general. In addition to that, having the results of classification, we would be able to tune the methods' hyperparameters for the best performing subset in order to calibrate the model, what would result in a better fit and enhanced performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bbb6e",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "We generate the instance of a classifier and then we train it using the cross validation algorithm. For each subset of features we are able to apply the evaluation metrics and present them in a form of a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197756b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03657c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_scores_1 = skm.cross_validate(model_XGB, features, labels, cv=5,scoring=scorings_class)\n",
    "XGB_accuracy_1 = np.mean(XGB_scores_1['test_accuracy'])\n",
    "XGB_roc_auc_1 = np.mean(XGB_scores_1['test_roc_auc'])\n",
    "XGB_precision_1 = np.mean(XGB_scores_1['test_precision'])\n",
    "XGB_recall_1 = np.mean(XGB_scores_1['test_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9734372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_scores_2 = skm.cross_validate(model_XGB, features_selected_10, labels_no_nan, cv=5,scoring=scorings_class)\n",
    "XGB_accuracy_2 = np.mean(XGB_scores_2['test_accuracy'])\n",
    "XGB_roc_auc_2 = np.mean(XGB_scores_2['test_roc_auc'])\n",
    "XGB_precision_2 = np.mean(XGB_scores_2['test_precision'])\n",
    "XGB_recall_2 = np.mean(XGB_scores_2['test_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72bfe77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_scores_3 = skm.cross_validate(model_XGB, features_pca, labels_no_nan, cv=5,scoring=scorings_class)\n",
    "XGB_accuracy_3 = np.mean(XGB_scores_3['test_accuracy'])\n",
    "XGB_roc_auc_3 = np.mean(XGB_scores_3['test_roc_auc'])\n",
    "XGB_precision_3 = np.mean(XGB_scores_3['test_precision'])\n",
    "XGB_recall_3 = np.mean(XGB_scores_3['test_recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81d8f7",
   "metadata": {},
   "source": [
    "In the table below, we can notics the numerical values for the metrics of quality assessment. It appears that the best performing subset is the first one, as for each column the values of evaluations are the highest with the accuracy reaching approximately 93% what can be considered as a good prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc32749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subset 1</th>\n",
       "      <td>0.932250</td>\n",
       "      <td>0.966074</td>\n",
       "      <td>0.882411</td>\n",
       "      <td>0.766728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset 2</th>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.878074</td>\n",
       "      <td>0.831397</td>\n",
       "      <td>0.533154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset 3</th>\n",
       "      <td>0.900310</td>\n",
       "      <td>0.922246</td>\n",
       "      <td>0.877634</td>\n",
       "      <td>0.587048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy       AUC  Precision    Recall\n",
       "subset 1  0.932250  0.966074   0.882411  0.766728\n",
       "subset 2  0.884211  0.878074   0.831397  0.533154\n",
       "subset 3  0.900310  0.922246   0.877634  0.587048"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_XGB_dict = {'Accuracy':[XGB_accuracy_1,XGB_accuracy_2,XGB_accuracy_3],\n",
    "              'AUC':[XGB_roc_auc_1,XGB_roc_auc_2,XGB_roc_auc_3],\n",
    "              'Precision':[XGB_precision_1,XGB_precision_2,XGB_precision_3],\n",
    "              'Recall':[XGB_recall_1,XGB_recall_2,XGB_recall_3]}\n",
    "scores_XGB_table = pd.DataFrame.from_dict(scores_XGB_dict, orient='index',\n",
    "                       columns=['subset 1', 'subset 2', 'subset 3']).transpose()\n",
    "scores_XGB_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4848e139",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "That way, as we mentioned before, we can choose the first subset and try to calibrate the model's hyperparameters in order to boost its performance. To do that we will focus on looking for the optimal values via the grid search method, where we can apply scoring methods and choose the values for which the model would be the most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b6914",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_XGB_to_tune = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', colsample_bylevel=0.9, colsample_bytree=0.1)\n",
    "\n",
    "param_grid = {'reg_alpha': [0,0.3,0.6,1,2],\n",
    "             'reg_lambda': [0,0.3,0.6,1,2]}\n",
    " \n",
    "grid = GridSearchCV(model_XGB_to_tune, param_grid, scoring = scorings_class, cv = 5, verbose = 3, refit = False)\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264cb4c",
   "metadata": {},
   "source": [
    "It turns out that adjusting parameters colsample_bylevel to 0.9 and colsample_bytree to 0.1 have boosted the performance and accuracy so that we can redo the training process inputting these parameters to the model and check whether there in fact will be any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee0808c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB_tuned = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', colsample_bylevel=0.9, colsample_bytree=0.1)\n",
    "XGB_scores_tuned = skm.cross_validate(model_XGB_tuned, features, labels, cv=5,scoring=scorings_class)\n",
    "XGB_accuracy_tuned = np.mean(XGB_scores_tuned['test_accuracy'])\n",
    "XGB_roc_auc_tuned = np.mean(XGB_scores_tuned['test_roc_auc'])\n",
    "XGB_precision_tuned = np.mean(XGB_scores_tuned['test_precision'])\n",
    "XGB_recall_tuned = np.mean(XGB_scores_tuned['test_recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b2c88",
   "metadata": {},
   "source": [
    "As we notice on the table below, all of the metrics' values have increased and we can claim that the tuning process was successful. After that we can move on and check the performance of the remaining methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c459c906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subset 1</th>\n",
       "      <td>0.94425</td>\n",
       "      <td>0.972501</td>\n",
       "      <td>0.924426</td>\n",
       "      <td>0.789035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy       AUC  Precision    Recall\n",
       "subset 1   0.94425  0.972501   0.924426  0.789035"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_XGB_dict_tuned = {'Accuracy':[XGB_accuracy_tuned],\n",
    "              'AUC':[XGB_roc_auc_tuned],\n",
    "              'Precision':[XGB_precision_tuned],\n",
    "              'Recall':[XGB_recall_tuned]}\n",
    "scores_XGB_table_tuned = pd.DataFrame.from_dict(scores_XGB_dict_tuned, orient='index',\n",
    "                       columns=['subset 1']).transpose()\n",
    "scores_XGB_table_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702dc77",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Similarly as before, we will follow the same training path utilizing similar algorithms and resources, what will allow us to check how this method behaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aa5a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = ct.CatBoostClassifier(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83c6eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_scores_1 = skm.cross_validate(model_cat, features, labels, cv=5,scoring=scorings_class)\n",
    "Cat_accuracy_1 = np.mean(Cat_scores_1['test_accuracy'])\n",
    "Cat_roc_auc_1 = np.mean(Cat_scores_1['test_roc_auc'])\n",
    "Cat_precision_1 = np.mean(Cat_scores_1['test_precision'])\n",
    "Cat_recall_1 = np.mean(Cat_scores_1['test_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcb70fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_scores_2 = skm.cross_validate(model_cat, features_selected_10, labels_no_nan, cv=5,scoring=scorings_class)\n",
    "Cat_accuracy_2 = np.mean(Cat_scores_2['test_accuracy'])\n",
    "Cat_roc_auc_2 = np.mean(Cat_scores_2['test_roc_auc'])\n",
    "Cat_precision_2 = np.mean(Cat_scores_2['test_precision'])\n",
    "Cat_recall_2 = np.mean(Cat_scores_2['test_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f16781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_scores_3 = skm.cross_validate(model_cat, features_pca, labels_no_nan, cv=5,scoring=scorings_class)\n",
    "Cat_accuracy_3 = np.mean(Cat_scores_3['test_accuracy'])\n",
    "Cat_roc_auc_3 = np.mean(Cat_scores_3['test_roc_auc'])\n",
    "Cat_precision_3 = np.mean(Cat_scores_3['test_precision'])\n",
    "Cat_recall_3 = np.mean(Cat_scores_3['test_recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38d5c5",
   "metadata": {},
   "source": [
    "The table indicates that this time once again the most accurate catboost is while applied for the first subset because all of the mentioned metrics take the highest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33a0e5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subset 1</th>\n",
       "      <td>0.939500</td>\n",
       "      <td>0.972630</td>\n",
       "      <td>0.938299</td>\n",
       "      <td>0.749398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset 2</th>\n",
       "      <td>0.890093</td>\n",
       "      <td>0.877535</td>\n",
       "      <td>0.895433</td>\n",
       "      <td>0.511568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset 3</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.886105</td>\n",
       "      <td>0.576243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy       AUC  Precision    Recall\n",
       "subset 1  0.939500  0.972630   0.938299  0.749398\n",
       "subset 2  0.890093  0.877535   0.895433  0.511568\n",
       "subset 3  0.900000  0.928736   0.886105  0.576243"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_Cat_dict = {'Accuracy':[Cat_accuracy_1,Cat_accuracy_2,Cat_accuracy_3],\n",
    "              'AUC':[Cat_roc_auc_1,Cat_roc_auc_2,Cat_roc_auc_3],\n",
    "              'Precision':[Cat_precision_1,Cat_precision_2,Cat_precision_3],\n",
    "              'Recall':[Cat_recall_1,Cat_recall_2,Cat_recall_3]}\n",
    "scores_Cat_table = pd.DataFrame.from_dict(scores_Cat_dict, orient='index',\n",
    "                       columns=['subset 1', 'subset 2', 'subset 3']).transpose()\n",
    "scores_Cat_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f64166",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "Also this time it is possible for us to find the optimal hyperparameters for the model using the grid search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c32def",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_Cat_to_tune = ct.CatBoostClassifier(verbose = False,iterations=500)\n",
    "param_grid = {'learning_rate': [0.01, 0.1,0.5,0.9],\n",
    "             'random_strength': [0.2,0.5,0.8]}\n",
    " \n",
    "grid_cat = GridSearchCV(model_Cat_to_tune, param_grid, scoring = scorings_class, cv = 5, verbose = 3, refit = False)\n",
    "grid_cat.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d30e9",
   "metadata": {},
   "source": [
    "It appears that no combination of different parameters could boost the performance. The only thing that allowed to achieve a little bit better evaluations was the increase in the number of iterations of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd0cb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Cat_tuned = ct.CatBoostClassifier(verbose = False, iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2511a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_scores_tuned = skm.cross_validate(model_Cat_tuned, features, labels, cv=5,scoring=scorings_class)\n",
    "Cat_accuracy_tuned = np.mean(Cat_scores_tuned['test_accuracy'])\n",
    "Cat_roc_auc_tuned = np.mean(Cat_scores_tuned['test_roc_auc'])\n",
    "Cat_precision_tuned = np.mean(Cat_scores_tuned['test_precision'])\n",
    "Cat_recall_tuned = np.mean(Cat_scores_tuned['test_recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe253988",
   "metadata": {},
   "source": [
    "The table below indicates that the accuracy along with remaining metrics is slightly better for the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04471ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subset 1</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.973182</td>\n",
       "      <td>0.941762</td>\n",
       "      <td>0.754344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy       AUC  Precision    Recall\n",
       "subset 1     0.941  0.973182   0.941762  0.754344"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_Cat_dict_tuned = {'Accuracy':[Cat_accuracy_tuned],\n",
    "              'AUC':[Cat_roc_auc_tuned],\n",
    "              'Precision':[Cat_precision_tuned],\n",
    "              'Recall':[Cat_recall_tuned]}\n",
    "scores_Cat_table_tuned = pd.DataFrame.from_dict(scores_Cat_dict_tuned, orient='index',\n",
    "                       columns=['subset 1']).transpose()\n",
    "scores_Cat_table_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d563c",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "Now we can consider the last of proposed methods, where the situation is pretty similar to the approaches used previously, where we apply the method to three subsets and check their performance using evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "254a448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGB = lgb.LGBMClassifier()\n",
    "LGB_scores_1 = skm.cross_validate(model_LGB, features, labels, cv=5,scoring=scorings_class)\n",
    "LGB_accuracy_1 = np.mean(LGB_scores_1['test_accuracy'])\n",
    "LGB_roc_auc_1 = np.mean(LGB_scores_1['test_roc_auc'])\n",
    "LGB_precision_1 = np.mean(LGB_scores_1['test_precision'])\n",
    "LGB_recall_1 = np.mean(LGB_scores_1['test_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7961863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_scores_2 = skm.cross_validate(model_LGB, features_selected_10, labels_no_nan, cv=5,scoring=scorings_class)\n",
    "LGB_accuracy_2 = np.mean(LGB_scores_2['test_accuracy'])\n",
    "LGB_roc_auc_2 = np.mean(LGB_scores_2['test_roc_auc'])\n",
    "LGB_precision_2 = np.mean(LGB_scores_2['test_precision'])\n",
    "LGB_recall_2 = np.mean(LGB_scores_2['test_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b8771aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_scores_3 = skm.cross_validate(model_LGB, features_pca, labels_no_nan, cv=5,scoring=scorings_class)\n",
    "LGB_accuracy_3 = np.mean(LGB_scores_3['test_accuracy'])\n",
    "LGB_roc_auc_3 = np.mean(LGB_scores_3['test_roc_auc'])\n",
    "LGB_precision_3 = np.mean(LGB_scores_3['test_precision'])\n",
    "LGB_recall_3 = np.mean(LGB_scores_3['test_recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18473f99",
   "metadata": {},
   "source": [
    "Unsurprisingly, the situation looks similar to the ones shown previously, where the best performing subset is the first one whose acuracy exceeds the level of 93%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "980b773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subset 1</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.969193</td>\n",
       "      <td>0.897921</td>\n",
       "      <td>0.779135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset 2</th>\n",
       "      <td>0.881115</td>\n",
       "      <td>0.876959</td>\n",
       "      <td>0.785137</td>\n",
       "      <td>0.562421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset 3</th>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.924984</td>\n",
       "      <td>0.870004</td>\n",
       "      <td>0.556243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy       AUC  Precision    Recall\n",
       "subset 1  0.937500  0.969193   0.897921  0.779135\n",
       "subset 2  0.881115  0.876959   0.785137  0.562421\n",
       "subset 3  0.894118  0.924984   0.870004  0.556243"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_LGB_dict = {'Accuracy':[LGB_accuracy_1,LGB_accuracy_2,LGB_accuracy_3],\n",
    "              'AUC':[LGB_roc_auc_1,LGB_roc_auc_2,LGB_roc_auc_3],\n",
    "              'Precision':[LGB_precision_1,LGB_precision_2,LGB_precision_3],\n",
    "              'Recall':[LGB_recall_1,LGB_recall_2,LGB_recall_3]}\n",
    "scores_LGB_table = pd.DataFrame.from_dict(scores_LGB_dict, orient='index',\n",
    "                       columns=['subset 1', 'subset 2', 'subset 3']).transpose()\n",
    "scores_LGB_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1adf1",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "We can try to improve the resutls by tuning the method's hyperparameters. It turns out that such approach did not provide any improvements so that we will have to stick with the primary models and their evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f25a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGB_to_tune = lgb.LGBMClassifier()\n",
    "param_grid = {'max_depth' : [10,50,100,200,500]\n",
    "             }\n",
    " \n",
    "grid_LGB = GridSearchCV(model_Cat_to_tune, param_grid, scoring = scorings_class, cv = 5, verbose = 3, refit = False)\n",
    "grid_LGB.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5c2ea",
   "metadata": {},
   "source": [
    "### Final model\n",
    "\n",
    "In order to predict the labels from the test data we have to pick one of the presented models and implement it. Judging them by the values of evaluation metrics the two best performing models were achieved for tuned XGBoost on first subset and for tuned Catboost also for first subset. The results are really close, but the most important factor appears to be the accuracy, so we will select XGBoost as the final and best model as it provides the highest predictive ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d73c1d",
   "metadata": {},
   "source": [
    "For the purpose of performing the final predictions we have to use the same functions on the test data in order to structurize and prepare it for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dd25d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups_onehot_test= group_processing(data_json_test)\n",
    "df_hobbies_onehot_test = hobbies_processing(df_csv_test)\n",
    "onehot_csv_test, numerical_csv_norm_test = data_processing(df_csv_test)\n",
    "merged_test = pd.concat([numerical_csv_norm_test, df_groups_onehot_test,df_hobbies_onehot_test,onehot_csv_test,pd.DataFrame(np.zeros(len(onehot_csv_test)))],axis = 1)\n",
    "features_test = np.array(merged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73bfcf",
   "metadata": {},
   "source": [
    "Then we can load the model and fit it on the whole training data containing the first subset of variables. For the task to be completed we can store the binary predictions of labels as well as the propensity probabilities in lists and then convert them to the final .csv file, where for each user ID these values are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd7d9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model_XGB_tuned\n",
    "best_model.fit(features, labels)\n",
    "predictions = best_model.predict(features_test)\n",
    "probabilities = [i[1] for i in best_model.predict_proba(features_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f39c0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = pd.DataFrame(df_csv_test['user_id'])\n",
    "to_save['probability_of_one'] = probabilities\n",
    "to_save['target'] = predictions\n",
    "to_save.set_index('user_id').to_csv(\"test_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37e038",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "#### Insights\n",
    "\n",
    "For international chain of gyms it is really crucial to analyse the behavior of potential new clients and subscribers, and for that reason the creation of different predictive models may allow the eficient selection of targets. The data concerning the surveyed people can be in fact very insightful and its correct preparation and processing is an essential part of building such models. Unfortunately, as there is some missing information about users, the quality of the data might be a little distorted and usually imputing the absent values does not make a point. Because of that, the range of machine learning methods used for the classification is limited. Nevertheless, the selection of three different algorithms and their application for three different subsets allowed for the valuable predictions. After comparing different evaluation metrics, where the accuracy exceeded 90% for some of the models, we were able to pick one of them and apply it for the provided test data, what resulted in the posterior predictions and probabilities, which ten were saved and stored in a final .csv file.\n",
    "\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "Such approach was optimal concerning the number of entries to the data, but if there were much more rows and users surveyed, the dimensionality of the prepared data could be considered to be an insurmountable obstacle, what would result in the prolongled computation time. The other thing that was mentioned before is the lack of some of the values for different attributes and people. Without taking proper steps while dealing with them it is probable that the final results would be inaccurate and inconsistent. Finally, the functions created in order to prepare the data for ML models work only for the structure of the data which was provided in training and test files. If there were many inputs where the distribution of the information varied across the files, it would be preferable to use different kind of approach, such as natural language processing or even the implementation of ML methods/API in order to scrap the needed data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
