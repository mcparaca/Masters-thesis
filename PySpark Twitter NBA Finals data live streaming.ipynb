{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5c28f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import udf,col\n",
    "import re \n",
    "import emoji\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9876\n",
    "KEY_WORD = 'nba'\n",
    "%matplotlib inline\n",
    "\n",
    "def cleaner(text):\n",
    "    no_emoji = emoji.get_emoji_regexp().sub(u'', text)\n",
    "    no_url = re.sub(r'http\\S+', '', no_emoji)\n",
    "    no_RT = re.sub(\"RT @[A-Za-z0-9_]+\",\"\", no_url)\n",
    "    no_mentions = re.sub(\"@[A-Za-z0-9_]+\",\"\", no_RT)\n",
    "    cleanest = re.sub(\":\",\"\", no_mentions)\n",
    "    list_punct=list(string.punctuation)\n",
    "    filtered = [''.join(c for c in s if c not in list_punct) for s in cleanest]\n",
    "    filtered = ''.join(filtered)\n",
    "    return filtered\n",
    "\n",
    "def wordToken(text):\n",
    "    splitted = nltk.word_tokenize(text)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filteredSentence = [w for w in splitted if not w in stop_words]\n",
    "    return filteredSentence\n",
    "\n",
    "def wordTokenize(text):\n",
    "    splitted = nltk.word_tokenize(text)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filteredSentence = [w for w in splitted if not w in stop_words]\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in filteredSentence]\n",
    "    finalLem = \" \".join(finalLem)\n",
    "    return finalLem\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment = sid_obj.polarity_scores(text)['compound']\n",
    "    if sentiment >= 0.05 :\n",
    "        sentiment_result='positive'\n",
    "    elif sentiment <= - 0.05 :\n",
    "        sentiment_result='negative'\n",
    "    else :\n",
    "        sentiment_result='neutral'\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def extractPhraseFunct(x):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    def leaves(tree):\n",
    "        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "    \n",
    "    def get_terms(tree):\n",
    "        for leaf in leaves(tree):\n",
    "            term = [w for w,t in leaf if not w in stop_words]\n",
    "            yield term\n",
    "\n",
    "    sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "    grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokens = nltk.regexp_tokenize(x,sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(tokens) #Part of speech tagging \n",
    "    tree = chunker.parse(postoks) #chunking\n",
    "    terms = get_terms(tree)\n",
    "    temp_phrases = []\n",
    "    for term in terms:\n",
    "        if len(term):\n",
    "            temp_phrases.append(' '.join(term))\n",
    "    \n",
    "    finalPhrase = [w for w in temp_phrases if w] #remove empty lists\n",
    "    return finalPhrase\n",
    "\n",
    "def word_counter(pddf):\n",
    "    \n",
    "    pd_df=pddf.toPandas()\n",
    "    wordlist = []\n",
    "    for i,tweet in enumerate(pd_df['tweet_split']):\n",
    "        tweet = tweet.replace(\"[\", \"\")\n",
    "        tweet = tweet.replace(\"]\", \"\")\n",
    "        tweet = tweet.replace(\"’\", \"\")\n",
    "        words = tweet.split(\", \")\n",
    "        wordlist.extend(words)\n",
    "    wordlist[:] = (value for value in wordlist if value != \"\")\n",
    "    result = Counter(wordlist).most_common()\n",
    "    scres = sc.parallelize(result)\n",
    "    df_fDist = scres.toDF()\n",
    "    df_fDist.createOrReplaceTempView(\"myTable\") \n",
    "    df2 = spark.sql(\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\") \n",
    "    pandD = df2.toPandas()\n",
    "    f1 = plt.figure()\n",
    "    f1.clear()\n",
    "    pandD.plot.barh(x='Keywords', y='Frequency', rot=1, figsize=(10,8))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def nba_finals(text):\n",
    "    warriors_words = [\"warriors\",\"golden\", \"state\", \"gsw\", \"dubs\", \"dubnation\",\n",
    "                          \"stephen\",\"curry\",\"steph\",\"klay\",\"thompson\",\"green\",\n",
    "                          \"draymond\", \"dray\",\"chase\",\"center\",\"poole\",\"looney\",\n",
    "                          \"kerr\", \"andrew\", \"wiggins\"]\n",
    "    boston_words = [\"boston\", \"celtics\", \"bos\", \"celts\", \"jayson\",\"tatum\",\"jaylen\", \"brown\",\n",
    "                        \"marcus\", \"smart\", \"al\", \"horford\", \"td\", \"garden\", \"ime\", \"udoka\",\n",
    "                        \"brad\", \"stevens\", \"grant\", \"williams\"]\n",
    "    finals_words = [\"finals\", \"nba\", \"nbafinals\", \"playoffs\", \"game\"]\n",
    "    text1 = text.lower()\n",
    "    sent = sentiment_analysis(text1)\n",
    "    var = None\n",
    "    for i in warriors_words:\n",
    "        if i in text1 and sent >0:\n",
    "            var = \"warriors positive\"\n",
    "            break\n",
    "        elif i in text1 and sent <0 :\n",
    "            var = \"warriors negative\"\n",
    "            break\n",
    "        elif i in text1 and sent == 0:\n",
    "            var = \"warriors neutral\"\n",
    "            break\n",
    "        \n",
    "    if var == None:\n",
    "        for j in boston_words:\n",
    "            if j in text1 and sent >0:\n",
    "                var = \"celtics positive\"\n",
    "                break\n",
    "            elif j in text1 and sent <0:\n",
    "                var = \"celtics negative\"\n",
    "                break\n",
    "            elif j in text1 and sent == 0:\n",
    "                var = \"celtics neutral\"\n",
    "                break\n",
    "    else: \n",
    "        for j in boston_words:\n",
    "            if j in text1 and sent >0:\n",
    "                var = \"warriors and celtics positive\"\n",
    "                break\n",
    "            elif j in text1 and sent <0:\n",
    "                var = \"warriors and celtics negative\"\n",
    "                break\n",
    "            elif j in text1 and sent == 0:\n",
    "                var = \"warriors and celtics neutral\"\n",
    "                break\n",
    "            \n",
    "    if var == None:\n",
    "        for l in finals_words:\n",
    "            if l in text1 and sent >0:\n",
    "                var = \"warriors and celtics positive\"\n",
    "                break\n",
    "            elif l in text1 and sent <0:\n",
    "                var = \"warriors and celtics negative\"\n",
    "                break\n",
    "            elif l in text1 and sent == 0:\n",
    "                var = \"warriors and celtics neutral\"\n",
    "                break\n",
    "    return var      \n",
    "     \n",
    "    \n",
    "    \n",
    "def spark(TCP_IP,TCP_PORT,KEY_WORD):\n",
    "    sc=SparkContext(appName=\"TwitterStreamming\")\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    ssc=StreamingContext(sc,10)\n",
    "    \n",
    "    socket_stream = ssc.socketTextStream(TCP_IP,TCP_PORT)\n",
    "    \n",
    "    lines=socket_stream.window(500)\n",
    "    def process(rdd):\n",
    "        \n",
    "        spark=SparkSession \\\n",
    "                .builder \\\n",
    "                .config(conf=rdd.context.getConf()) \\\n",
    "                .getOrCreate()\n",
    "        rowRdd = rdd.map(lambda x: Row(word=x))\n",
    "\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "        wordsDataFrame1 = wordsDataFrame.where(wordsDataFrame.word != '')\n",
    "\n",
    "        udf_cleaner = udf(lambda x: cleaner(x))\n",
    "        udf_tokenizer = udf(lambda x: wordTokenize(x))\n",
    "        udf_splitter = udf(lambda x: wordToken(x))\n",
    "        udf_sentiment = udf(lambda x: sentiment_analysis(x))\n",
    "        udf_phrase = udf(lambda x: extractPhraseFunct(x))\n",
    "        udf_teams = udf(lambda x: nba_finals(x))\n",
    "        wordsDataFrame2= wordsDataFrame1.withColumn(\"tweet_content\",udf_cleaner(col(\"word\"))).select(\"tweet_content\")\n",
    "        wordsDataFrame_clean = wordsDataFrame2.where(wordsDataFrame2.tweet_content != '')\n",
    "        wordsDataFrame_clean_split = wordsDataFrame_clean.withColumn(\"tweet_split\",udf_splitter(col(\"tweet_content\"))).select(\"tweet_content\",\"tweet_split\")\n",
    "        wordsDataFrame_clean_tok = wordsDataFrame_clean.withColumn(\"tweet_tokenized\",udf_tokenizer(col(\"tweet_content\"))).select(\"tweet_content\",\"tweet_tokenized\")\n",
    "        wordsDataFrame_clean_tok_sent = wordsDataFrame_clean_tok.withColumn(\"sentiment_score\",udf_sentiment(col(\"tweet_tokenized\"))).select(\"tweet_tokenized\",\"sentiment_score\")\n",
    "        wordsDataFrame_teams = wordsDataFrame_clean_tok.withColumn(\"sentiment_team\",udf_teams(col(\"tweet_tokenized\"))).select(\"tweet_tokenized\",\"sentiment_team\")        \n",
    "        \n",
    "        tokenizer = Tokenizer(inputCol=\"tweet_content\", outputCol=\"tweet_words\")\n",
    "        wordsData = tokenizer.transform(wordsDataFrame_clean)\n",
    "        \n",
    "        print(\"The graphs for the next batch: \\n \\n \\n\")\n",
    "        \n",
    "\n",
    "\n",
    "        wordsDataFrame_clean_split.createOrReplaceTempView(\"splits\")\n",
    "        wordCountsDataFrame = spark.sql(\"select tweet_split from splits\")\n",
    "        pd_df=wordCountsDataFrame.toPandas()\n",
    "        wordlist = []\n",
    "        for i,tweet in enumerate(pd_df['tweet_split']):\n",
    "            tweet = tweet.lower()\n",
    "            tweet = tweet.replace(\"[\", \"\")\n",
    "            tweet = tweet.replace(\"]\", \"\")\n",
    "            tweet = tweet.replace(\"’\", \"\")\n",
    "            words = tweet.split(\", \")\n",
    "            wordlist.extend(words)\n",
    "        wordlist[:] = (value for value in wordlist if value != \"\")\n",
    "        result = Counter(wordlist).most_common(20)\n",
    "        columns = ['word','frequency']\n",
    "        df_1word = pd.DataFrame([x for x in result], columns=columns)\n",
    "        f1 = plt.figure()\n",
    "        df_1word.plot.barh(x='word', y='frequency', rot=1, figsize=(10,8))\n",
    "        plt.title(\"Frequency of words in tweets\")\n",
    "        plt.show()\n",
    "        \n",
    "        ngram2model = NGram(inputCol=\"tweet_words\", outputCol=\"ngrams\")\n",
    "        ngram2 = ngram2model.transform(wordsData).select('ngrams')      \n",
    "        ngram2.createOrReplaceTempView(\"ngr\")\n",
    "        ngramsCountsDataFrame = spark.sql(\"select ngrams from ngr\")\n",
    "        pd_ngram=ngramsCountsDataFrame.toPandas()\n",
    "        gramlist = []\n",
    "        for kk,gram in enumerate(pd_ngram['ngrams']):\n",
    "            grama = gram[1:len(gram)-1]\n",
    "            gramlist.extend(grama)\n",
    "        gramlist[:] = (value for value in gramlist if value != \"\")\n",
    "        result_g = Counter(gramlist).most_common(15)\n",
    "        columns2 = ['twograms','frequency']\n",
    "        df_2word = pd.DataFrame([x for x in result_g], columns=columns2)\n",
    "        f3 = plt.figure()\n",
    "        df_2word.plot.barh(x='twograms', y='frequency', rot=1, figsize=(10,8))\n",
    "        plt.title(\"Frequency of 2grams in tweets\")\n",
    "        plt.show()\n",
    "        \n",
    "        ngram3model = NGram(n=3, inputCol=\"tweet_words\", outputCol=\"ngrams\")\n",
    "        ngram3 = ngram3model.transform(wordsData).select('ngrams')\n",
    "        ngram3.createOrReplaceTempView(\"ngr3\")\n",
    "        ngramsCountsDataFrame3 = spark.sql(\"select ngrams from ngr3\")\n",
    "        pd_ngram3=ngramsCountsDataFrame3.toPandas()\n",
    "        gramlist3 = []\n",
    "        for kk,gram3 in enumerate(pd_ngram3['ngrams']):\n",
    "            gram3a = gram3[1:len(gram3)-1]\n",
    "            gramlist3.extend(gram3a)\n",
    "        gramlist3[:] = (value for value in gramlist3 if value != \"\")\n",
    "        result_g3 = Counter(gramlist3).most_common(15)\n",
    "        columns3 = ['threegrams','frequency']\n",
    "        df_3word = pd.DataFrame([x for x in result_g3], columns=columns3)\n",
    "        f4 = plt.figure()\n",
    "        df_3word.plot.barh(x='threegrams', y='frequency', rot=1, figsize=(10,8))\n",
    "        plt.title(\"Frequency of 3grams in tweets\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        wordsDataFrame_clean_tok_sent.createOrReplaceTempView(\"sent\")\n",
    "        wordSentDataFrame = spark.sql(\"select sentiment_score from sent\")\n",
    "        pd_sent=wordSentDataFrame.toPandas()\n",
    "        scores = []\n",
    "        for j,score in enumerate(pd_sent['sentiment_score']):\n",
    "            scores.append(float(score))\n",
    "        bins = np.linspace(-1, 1, 10)\n",
    "        f2 = plt.figure()\n",
    "        plt.hist(scores, bins=bins)\n",
    "        plt.axvline(x=np.mean(scores), color='r')\n",
    "        plt.title('Histogram of sentiment score for each tweet')\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Value')\n",
    "        plt.show()\n",
    "\n",
    "        wordsDataFrame_teams_nn = wordsDataFrame_teams.where(col(\"sentiment_team\").isNotNull())\n",
    "        num = np.round(wordsDataFrame_teams_nn.count()/wordsDataFrame_teams.count(),3)*100\n",
    "        wordsDataFrame_teams_nn.createOrReplaceTempView(\"teams\")\n",
    "        wordTeamsDataFrame = spark.sql(\"select  sentiment_team, count(sentiment_team) as sentiment from teams group by sentiment_team\")\n",
    "        pd_teams=wordTeamsDataFrame.toPandas()\n",
    "        f5 = plt.figure()\n",
    "        pd_teams.groupby(['sentiment_team']).sum().plot(kind='pie', y='sentiment', figsize=(5, 5),legend = False)\n",
    "        plt.title(f\"The {num} percent of tweets deal with the finals' rivarly\")\n",
    "        plt.show()\n",
    "    lines.foreachRDD(process)\n",
    "    ssc.start()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    spark(TCP_IP,TCP_PORT,KEY_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52799e7f",
   "metadata": {},
   "source": [
    "### Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c88b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import udf,col\n",
    "import re \n",
    "import emoji\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def spark(TCP_IP,TCP_PORT):\n",
    "    sc=SparkContext(appName=\"TwitterStreamming\")\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    ssc=StreamingContext(sc,5)\n",
    "    \n",
    "    socket_stream = ssc.socketTextStream(TCP_IP,TCP_PORT)\n",
    "    \n",
    "    lines=socket_stream.window(300)\n",
    "    df=lines.flatMap(lambda x:x.split(\" \"))  \\\n",
    "            .filter(lambda x:x.startswith(\"#\")) \n",
    "    \n",
    "    def process(rdd):\n",
    "        spark=SparkSession \\\n",
    "                .builder \\\n",
    "                .config(conf=rdd.context.getConf()) \\\n",
    "                .getOrCreate()\n",
    "    \n",
    "        rowRdd = rdd.map(lambda x: Row(word=x))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "    \n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "        wordCountsDataFrame = spark.sql(\"select word, count(*) as total from words group by word order by 2 desc\")       \n",
    "        pd_df=wordCountsDataFrame.toPandas()\n",
    "        \n",
    "        plt.figure( figsize = ( 10, 8 ) )\n",
    "        sns.barplot( x=\"total\", y=\"word\", data=pd_df.head(20))\n",
    "        plt.title(\"Most popular hashtags\")\n",
    "        plt.show()\n",
    "        \n",
    "    df.foreachRDD(process)\n",
    "    \n",
    "    ssc.start()\n",
    "    \n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9876  \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    spark(TCP_IP,TCP_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d77c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
